# -*- coding: utf-8 -*-
"""Womens Ecommerce_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/159JdgDx1VVs-n8LOavNJoSujL3KTxfgB
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""# Data Preprocessing"""

dataset = pd.read_csv("Womens Clothing E-Commerce Reviews.csv")
dataset

"""## Fill the nan with fillna"""

dataset['Review Text'] = dataset['Review Text'].fillna('')

dataset.duplicated().sum()

dataset.info()

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""# Tokenization, Removing Stopwords and Lemmatization"""

# Select relevant columns
reviews = dataset['Review Text']
labels = dataset['Recommended IND']

# Initialize preprocessing tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Tokenize text
    tokens = word_tokenize(text)
    # Remove stopwords and non-alphabetic tokens, and apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)

processed_reviews = reviews.apply(preprocess_text)

"""## Cleaned Text"""

import string
import re

def clean_text(text):
    return re.sub('[^a-zA-Z]', ' ', text).lower()
dataset['cleaned_text'] = dataset['Review Text'].apply(lambda x: clean_text(x))

"""## Tokenization"""

def tokenize_text(text):
    tokenized_text = text.split()
    return tokenized_text
dataset['tokens'] = dataset['cleaned_text'].apply(lambda x: tokenize_text(x))
dataset.head()

"""## Lemmatization"""

def lemmatize_text(token_list):
    return " ".join([lemmatizer.lemmatize(token) for token in token_list if not token in set(stop_words)])

dataset['lemmatized_review'] = dataset['tokens'].apply(lambda x: lemmatize_text(x))
dataset.head()

"""## Visualizing Word Clouds

"""

df_negative = dataset[dataset['Recommended IND'] == 0]
df_positive = dataset[dataset['Recommended IND'] == 1]

# Convert to lists
negative_list = df_negative['lemmatized_review'].tolist()
positive_list = df_positive['lemmatized_review'].tolist()

filtered_negative = ("").join(str(negative_list))
# Convert the list into a string
filtered_negative = filtered_negative.lower()

filtered_positive = ("").join(str(positive_list))
# Convert the list into a string
filtered_positive = filtered_positive.lower()

pip install wordcloud

import matplotlib.pyplot as plt

from wordcloud import WordCloud, STOPWORDS

wordcloud = WordCloud(background_color='lightblue',
                      width=1600, height=800,
                      max_font_size=200).generate(filtered_positive)
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.title('Positive Reviews', fontsize = 20)
plt.axis("off")
plt.show()

wordcloud = WordCloud(width=1600, height=800,
                      max_font_size=200).generate(filtered_negative)
plt.figure(figsize=(12, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.title('Negative Reviews', fontsize = 20)
plt.axis("off")
plt.show()

dataset['Recommended IND'].value_counts().plot(kind='bar')
plt.title("label of sentiment", fontsize = 10)
plt.xlabel("label", fontsize = 10)
plt.xticks(rotation=0)
plt.ylabel("Count", fontsize = 10)
plt.figure(figsize=(7,6))

"""# Split Data"""



X_train, X_test, y_train, y_test = train_test_split(
    processed_reviews, labels, test_size=0.2, stratify=labels, random_state=42
)

"""# Text Preprocessing and Feature Extraction"""

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

"""# Train Model"""

model = LogisticRegression(random_state=42)
model.fit(X_train_tfidf, y_train)

"""# Evaluate Model"""

y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print Results
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:")
print(classification_rep)
print("\nConfusion Matrix:")
print(conf_matrix)

